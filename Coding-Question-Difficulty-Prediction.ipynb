{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Question Difficulty Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a project with the aim of predicting difficulty in answering competitive coding questions using labelled data from [codeforces.com](http://codeforces.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It classifies problems into three levels of difficulty- Easy, Medium, and Hard from a dataset of around 2000 questions containing - The question text, the input, and the output specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import keras.utils\n",
    "\n",
    "import nltk\n",
    "import glob\n",
    "import os.path\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lower(string):\n",
    "    return \" \".join(nltk.word_tokenize(string)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    '''\n",
    "    reads all question files and returns - \n",
    "    q_text, input, output, label\n",
    "    '''\n",
    "\n",
    "    complexities = dict()\n",
    "    complexity_file = open(\"questions-complexity.csv\", encoding='utf-8')\n",
    "    complexity_file.readline()\n",
    "\n",
    "    for line in complexity_file:\n",
    "        line = line.strip().split(\",\")\n",
    "        complexities[line[0]] = line[-2]\n",
    "    complexity_file.close()\n",
    "\n",
    "\n",
    "    question_files = sorted(glob.glob(\"./questions/*.txt\"))\n",
    "    questions = []\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    labels = []\n",
    "\n",
    "    for f in question_files:\n",
    "        handle = open(f, encoding='utf-8')\n",
    "        text = handle.read()\n",
    "        handle.close()\n",
    "        text_split = text.split(\"\\n\\n\")\n",
    "        question = text_split[2]\t### + [3] and [4] for Input and Output requirement text\n",
    "        input_text = text_split[3]\n",
    "        output_text = text_split[4]\n",
    "\n",
    "        # Removes 'Input' and 'Output' prefixes\n",
    "        input_text = input_text[len('Input'):]\n",
    "        output_text = output_text[len('Output'):]\n",
    "\n",
    "        question = tokenize_and_lower(question)\n",
    "        input_text = tokenize_and_lower(input_text)\n",
    "        output_text = tokenize_and_lower(output_text)\n",
    "\n",
    "        questions.append(question)\n",
    "        inputs.append(input_text)\n",
    "        outputs.append(output_text)\n",
    "        labels.append(complexities[os.path.basename(f).strip(\".txt\")])\n",
    "\n",
    "    return questions, inputs, outputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_word_frequencies(word_freq, text):\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        if token not in word_freq:\n",
    "            word_freq[token] = 0\n",
    "        else:\n",
    "            word_freq[token] += 1\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(questions, inputs, outputs):\n",
    "    '''\n",
    "    takes list of questions (returned from load_data) and returns a list of words sorted in desending order of frequency\n",
    "    '''\n",
    "    word_freq = dict()\n",
    "    for ques in questions:\n",
    "        populate_word_frequencies(word_freq, ques)\n",
    "            \n",
    "    for input_text in inputs:\n",
    "        populate_word_frequencies(word_freq, input_text)\n",
    "        \n",
    "    for output_text in outputs:\n",
    "        populate_word_frequencies(word_freq, output_text)\n",
    "        \n",
    "    top_words = sorted(word_freq, key = lambda w : word_freq[w], reverse = True)\n",
    "    vocab_size = len(top_words)\n",
    "\n",
    "    return top_words, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frequency_vector(text, vocab):\n",
    "    vector = []\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        vector.append(vocab.index(token))\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(questions, labels, vocab):\n",
    "    question_vectors = []\n",
    "    input_vectors = []\n",
    "    output_vectors = []\n",
    "    \n",
    "    for ques in questions:\n",
    "        question_vectors.append(get_frequency_vector(ques, vocab))\n",
    "\n",
    "    for input_text in inputs:\n",
    "        input_vectors.append(get_frequency_vector(input_text, vocab))\n",
    "        \n",
    "    for output_text in outputs:\n",
    "        output_vectors.append(get_frequency_vector(output_text, vocab))\n",
    "        \n",
    "    labels = [0 if l == \"Easy\" else l for l in labels]\n",
    "    labels = [1 if l == \"Medium\" else l for l in labels]\n",
    "    labels = [2 if l == \"Hard\" else l for l in labels]\n",
    "\n",
    "    labels_vector = keras.utils.to_categorical(labels)\n",
    "\n",
    "    return question_vectors, input_vectors, output_vectors, labels_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, inputs, outputs, labels = load_data()\n",
    "vocab, vocab_size = create_vocab(questions, inputs, outputs)\n",
    "question_vectors, input_vectors, output_vectors, labels = vectorize_data(questions, labels, vocab)\n",
    "\n",
    "question_vectors_train, input_vectors_train, output_vectors_train, labels_train = question_vectors[:1700], input_vectors[:1700], output_vectors[:1700], labels[:1700]\n",
    "question_vectors_test, input_vectors_test, output_vectors_test, labels_test = question_vectors[1700:], input_vectors[1700:], output_vectors[1700:], labels[1700:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_length = 300\n",
    "max_input_length = 50\n",
    "max_output_length = 50\n",
    "embedding_vector_length = 32\n",
    "n_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(question_vectors_train, input_vectors_train, output_vectors_train, labels_train, vocab_size, max_question_length, max_input_length, max_output_length, embedding_vector_length, n_epochs):\n",
    "    question_vectors_sequence = sequence.pad_sequences(question_vectors_train, maxlen=max_question_length)\n",
    "    input_vectors_sequence = sequence.pad_sequences(input_vectors_train, maxlen=max_input_length)\n",
    "    output_vectors_sequence = sequence.pad_sequences(output_vectors_train, maxlen=max_output_length)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_vector_length))\n",
    "\n",
    "    model.add(LSTM(10))\n",
    "\n",
    "    model.add(Dense(3, activation = \"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "    X = np.concatenate([question_vectors_sequence, input_vectors_sequence, output_vectors_sequence], axis=1)\n",
    "    model.fit(X, labels_train, epochs=n_epochs, batch_size=64, verbose=1)\n",
    "\n",
    "    print('SUMMARY:', model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700,)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(question_vectors_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1700/1700 [==============================] - 9s 5ms/step - loss: 1.0931 - acc: 0.3718\n",
      "Epoch 2/5\n",
      "1700/1700 [==============================] - 7s 4ms/step - loss: 1.0734 - acc: 0.4282\n",
      "Epoch 3/5\n",
      " 640/1700 [==========>...................] - ETA: 4s - loss: 1.0427 - acc: 0.4516"
     ]
    }
   ],
   "source": [
    "model = build_model(question_vectors_train, input_vectors_train, output_vectors_train, labels_train, vocab_size, max_question_length, max_input_length, max_output_length, embedding_vector_length, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, question_vectors_test, input_vectors_test, output_vectors_test, labels_test, max_question_length, max_input_length, max_output_length):\n",
    "    question_vectors_sequence = sequence.pad_sequences(question_vectors_test, maxlen=max_question_length)\n",
    "    input_vectors_sequence = sequence.pad_sequences(input_vectors_test, maxlen=max_input_length)\n",
    "    output_vectors_sequence = sequence.pad_sequences(output_vectors_test, maxlen=max_output_length)\n",
    "\n",
    "    X = np.concatenate([question_vectors_sequence, input_vectors_sequence, output_vectors_sequence], axis=1)\n",
    "    scores = model.evaluate(X, labels_test)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, question_vectors_test, input_vectors_test, output_vectors_test, labels_test, max_question_length, max_input_length, max_output_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
